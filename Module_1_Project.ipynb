{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatiron School Data Science Immersive \n",
    "# Module 1 Project \n",
    "## Multivariate Analysis: Home Prices in King County, WA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Valentina C. Fontiveros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"house_banner.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To use data science tools to complete a multivariate analysis of factors influencing home prices in King County, WA.\n",
    "\n",
    "> To demonstrate the skills gained after completing Module 1 of the Data Science Immersive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset* used in this analysis records physical characteristics and prices of approximately twenty thousand housing units sold in King County, WA during 2014 and 2015.\n",
    "\n",
    "> Seattle, a major tech hub in the world, is located in this county.\n",
    "\n",
    "> As of 2010, it was the most densely-populated county in Washington.\n",
    "\n",
    "> *Flatiron School has provided and modified the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Population Density of Washington State counties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"King_County.jpg\" width=400 align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis | OSEMiN Framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On this project, we will use the OSEM Data Science framework.\n",
    "\n",
    "> OSEM stands for:\n",
    "    > Obtain \n",
    "    > Scrub \n",
    "    > Explore \n",
    "    > Model \n",
    "    > Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The libraries used provide useful tools for data science workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from scipy.stats import kurtosis, skew\n",
    "import folium\n",
    "from folium import plugins\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain and Sort Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Set index to ID\n",
    "> Ordered by price,zipcode,then ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kc_house_data.csv') # dataset provided by Flatiron School\n",
    "df.set_index('id', inplace=True)\n",
    "df.sort_values(by =['price','zipcode','id'], inplace=True, ascending=True)\n",
    "df.head() #preview data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are a number of columns which were not \n",
    "visible on the preview display. Let's display all the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are ' + str(len(df.columns)) + ' columns in the dataframe.')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We see 20 columns of information, describing various attributes of houses in King County."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrub Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House Prices:  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price - Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate histogram for price containing all data\n",
    "prices = df.price/1000\n",
    "prices.hist(label = 'prices', bins=50,density=True,color='green',alpha=0.4)\n",
    "prices.plot.kde(label='kde',color = 'green')\n",
    "plt.title('Sales Price for Houses in King County (2014-2015)')\n",
    "plt.xlabel('Prices (in 1000s)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print ('Skewness =', skew(df.price))\n",
    "print ('kurtosis =', kurtosis(df.price))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: high positive skew, extremely high + curtosis, long-tailed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price - Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statitics\n",
    "df.price.describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: ~ 20,000 samples, high standard deviation / variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot outliers\n",
    "plt.boxplot(df.price/1000)\n",
    "plt.title('Distribution of House Prices (in 1000s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: numerous outliers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Key Findings About Price:\n",
    "Entire Dataset\n",
    "> Number of houses:                 21,597 \n",
    "> Minimum house price is:    $      78,000\n",
    "> Median house price is:     $     450,000\n",
    "> Mean house price is:       $     540,296\n",
    "> Maximum house price is:    $   7,700,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price - Store in Dataframe after log normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the log of price and store in\n",
    "price_df = pd.DataFrame(np.log(df.price))\n",
    "price_df.hist(bins=15,color='green',alpha=0.4)\n",
    "print ('Skewness =', skew(price_df.price))\n",
    "print ('kurtosis =', kurtosis(price_df.price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After log normalizing, the skewness and kurtosis improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price - Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: price is a float, which is appropriate due to the log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features - Store in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for features\n",
    "features = df.columns.drop(['price'])\n",
    "features_df = pd.DataFrame(df[features])\n",
    "features_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Features: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain column info\n",
    "features_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "> There are numerous variables that should be tranformed to categories.\n",
    "\n",
    "> There are geographical features that would be interesting to display on a map.\n",
    "\n",
    "> Some variables could be transformed from float to int for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan features to determine best transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function scans features dataframe for unique values\n",
    "\n",
    "# Even though this output is large, scanning all the data is important\n",
    "# if we are not familiar with its peculiarities.\n",
    "\n",
    "def unique(features_df):\n",
    "    for column in features_df:\n",
    "        print('Name: ' + column)\n",
    "        print('-'*10)\n",
    "        na = features_df[column].isna().sum()\n",
    "        print('NaN values: ' + str(na))\n",
    "        print('-'*10)\n",
    "        unique = features_df[column].unique()\n",
    "\n",
    "        print(unique)\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see above than none of the categorical variables are identified as such.\n",
    "> We need to transform the type for several of these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are problems with various variables.\n",
    "Address those one variable at a time below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 'sqft_basement' feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature contains unknown values marked with a \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore how many ? entries are in sqft_basement.\n",
    "value = '?'\n",
    "count = 0\n",
    "for row in features_df['sqft_basement']:\n",
    "    if row == '?':\n",
    "        count +=1\n",
    "print('There are ' + str(count) + ' rows with question marks as unknown values.')\n",
    "print('This represents ' + str(round(count/len(features_df)*100,2)) + ' percent of the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0\n",
    "count = 0\n",
    "for row in features_df['sqft_basement']:\n",
    "    if row == '?':\n",
    "        count +=1\n",
    "print('There are ' + str(count) + ' rows with question marks as unknown values.')\n",
    "print('This represents ' + str(round(count/len(features_df)*100,2)) + ' percent of the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are several ways to transform the unknown ? entries. We could drop these rows or replace them with the median. \n",
    "\n",
    "> Personally, I prefer replacing with the median to begin with, which would introduce a neglible amount of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we can't calculate the mean of the dataset with strings, let's create a dummy\n",
    "# variable to replace the question mark with first.\n",
    "value = '?'\n",
    "dummy = 99999\n",
    "############# . df.loc[:,\"Score1\"].median()\n",
    "\n",
    "features_df.loc[:,'sqft_basement'].replace(value, dummy,inplace=True) \n",
    "\n",
    "# Now let's convert the type to int\n",
    "features_df['sqft_basement'] = features_df['sqft_basement'].map(lambda x: float(x))\n",
    "features_df['sqft_basement'] = features_df['sqft_basement'].map(lambda x: int(x))\n",
    "features_df['sqft_basement'].unique()[0:15]\n",
    "\n",
    "# Now let's replace 99999 with the median\n",
    "features_df.loc[:,'sqft_basement'].replace(dummy,np.nan, inplace=True) \n",
    "\n",
    "# Let's plot a histogram \n",
    "features_df['sqft_basement'].hist(bins=30,color='green',alpha=0.4)\n",
    "print('The median value for this basement area is: ' + str(features_df['sqft_basement'].median()))\n",
    "print('The mean value for this basement area is: ' + str(features_df['sqft_basement'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the median for known basement values is zero, we will replace the unknown ? values\n",
    "> with zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace original '?' entries with zeros.\n",
    "median_sqft = features_df['sqft_basement'].median()\n",
    "features_df['sqft_basement']= features_df['sqft_basement'].replace(np.nan, median_sqft) \n",
    "features_df['sqft_basement'].unique()[0:10]\n",
    "features_df.sqft_basement = features_df.sqft_basement.astype(int)\n",
    "print('Done replacing \"?\" with zeros in sqft_basement')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 'date' feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a column for the year sold based on 'date'. Reformat to integer.\n",
    "> We will assume that the exact day of the sale is not relevant to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute two new features: yr_sold and month_sold\n",
    "features_df['yr_sold'] = features_df['date'].map(lambda x: x.split('/',3))\n",
    "features_df['month_sold'] = features_df['yr_sold'].map(lambda x: int(x[0]))\n",
    "features_df['yr_sold'] = features_df['yr_sold'].map(lambda x: int(x[2]))\n",
    "\n",
    "print('Years of reported sales:')\n",
    "print(features_df['yr_sold'].unique())\n",
    "print('Months of reported sales:')\n",
    "print(features_df['month_sold'].unique())\n",
    "#features_df.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore how many homes were sold in 2014 and 2015\n",
    "features_df.yr_sold.hist(color='green',alpha=.4)\n",
    "plt.xlabel('Year Sold')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Homes Sold in 2014, 2015')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observations: Sales decreased by nearly 50% in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out whether mean/median prices changed from one year to the next.\n",
    "temp_price = pd.DataFrame(data=df.price)\n",
    "temp_df = temp_price.join(features_df, how='left')\n",
    "plt.scatter(temp_df.yr_sold,temp_df.price/1000,color='green',alpha=0.4)\n",
    "plt.scatter(2014, temp_df.price[temp_df.yr_sold == 2014].mean()/1000, color='indigo',label='Mean Price') \n",
    "plt.scatter(2015, temp_df.price[temp_df.yr_sold == 2015].mean()/1000, color='indigo') \n",
    "print('Mean price in 2014: ' + str(round(temp_df.price[temp_df.yr_sold==2014].mean(),0)))\n",
    "print('Mean price in 2015: ' + str(round(temp_df.price[temp_df.yr_sold==2015].mean(),0)))\n",
    "plt.title('Home Prices in 2014, 2015')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Price (in 1000s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No significant differences are apparent in value of home prices between 2014 and 2015. \n",
    "> This probably suggests that this data has been edited for the purposes of this project.\n",
    "> There is no clear discrimination of price year after year, even though 2014 contains most of the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.month_sold.hist(color='green',alpha=.4,bins=12)\n",
    "plt.title('Home Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observations: Sales pick up in the Spring, peak in May and slow down in late Fall and Winter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(temp_df.month_sold,temp_df.price/1000,color='green',alpha=0.4,label='Prices')\n",
    "for x in range(1,12):\n",
    "    plt.scatter(x, temp_df.price[temp_df.month_sold == x].mean()/1000, color='indigo')\n",
    "    print(temp_df.price[temp_df.month_sold == x].mean())\n",
    "plt.scatter(12, temp_df.price[temp_df.month_sold == 12].mean()/1000, color='indigo',label='Mean Sales Price')    \n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Price (in 1000s)')\n",
    "plt.title('Home Prices by Monthly Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observations: Mean/Median home values do not change month to month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Conclusion: There is no value in adding these features to the model\n",
    "> since they do not explain price variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant information\n",
    "\n",
    "features_df.drop('yr_sold',axis=1,inplace=True)\n",
    "features_df.drop('month_sold',axis=1,inplace=True)\n",
    "features_df.drop('date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Correlated Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding correlations between variables is important at this stage to keep\n",
    "in mind for the later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dataframe pinpointed correlated variables\n",
    "# that exceed a correlation threshhold. \n",
    "def find_corr_vars(df, threshhold):\n",
    "    \n",
    "    threshhold = threshhold\n",
    "    corr_vars0 = df.corr()\n",
    "    print('Numbers of variables tested')\n",
    "    print(len(corr_vars0.columns))\n",
    "    corr_vars = (corr_vars0 > threshhold)\n",
    "    length = len(corr_vars0.columns)\n",
    "    print('Correlations tested:')\n",
    "    print(corr_vars.size)\n",
    "    print('Correlations meeting threshhold:')\n",
    "    print(corr_vars.sum().sum() - length)\n",
    "\n",
    "    rows=[]\n",
    "    columns=[]\n",
    "    for column in range(0,length):\n",
    "        for row in range(0,length):\n",
    "            if corr_vars.iloc[row][column] == True:\n",
    "                rows.append(row)\n",
    "                columns.append(column)\n",
    "    corr1 = []\n",
    "    corr2 = []\n",
    "    words = list(corr_vars.columns)\n",
    "    for index in rows:\n",
    "        word = words[index]\n",
    "        corr1.append(word)\n",
    "    for index2 in columns:\n",
    "        corr2.append(words[index2])\n",
    "\n",
    "    corr_df = pd.DataFrame(data={'col1': corr1,'col2':corr2})\n",
    "    print('')\n",
    "\n",
    "    print('Correlated Variables over threshhold ' + str(threshhold))\n",
    "\n",
    "    return corr_df[corr_df.col1 != corr_df.col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold = 0.75 #set correlation threshhold minimum to filter correlations.\n",
    "# let's find correlated variables above a certain threshhold\n",
    "feat_price_df = features_df.join(price_df, how='left')\n",
    "find_corr_vars(feat_price_df, threshhold).sort_values(by='col1',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observation: We see here that sqft_living and correlated variables\n",
    "> have a marked correlation with price even before processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Categorical Variables / Reformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 'sqft_basement' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.sqft_basement,price_df.price,color='green',alpha=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, this variable was transformed to a category.\n",
    "# For now, we will leave 'as is'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 'yr_sold' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropped from analysis due to nil contribution to price variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 'bathrooms' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.bathrooms,price_df.price,color='green',alpha=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initially, this variable was transformed to a category.\n",
    "# For now, we will leave 'as is'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 'waterfront' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(temp_df.waterfront,temp_df.price,color='green',alpha=.4)\n",
    "plt.scatter(0, (temp_df.price[temp_df.waterfront==0]).mean(),color='indigo')\n",
    "plt.scatter(1, (temp_df.price[temp_df.waterfront==1]).mean(),color='indigo')\n",
    "plt.xlabel('Waterfront Property?, 0=No, 1=Yes')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Waterfront View Impact on Mean Price')\n",
    "print('Waterfront Mean Price: ' + str(round((temp_df.price[temp_df.waterfront==1]).mean(),0)))\n",
    "print('No Waterfront Mean Price: ' + str(round((temp_df.price[temp_df.waterfront==0]).mean(),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Waterfront properties are valued over a million dollars higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values in waterfront and view with zeros.\n",
    "features_df.waterfront.fillna(0,inplace=True) # 10% of data was missing.\n",
    "features_df['waterfront'] = features_df['waterfront'].astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that changes did not impact distribution\n",
    "temp_df.waterfront.fillna(0,inplace=True)\n",
    "plt.scatter(temp_df.waterfront,temp_df.price,color='green',alpha=.4)\n",
    "plt.scatter(0, (temp_df.price[temp_df.waterfront==0]).mean(),color='indigo')\n",
    "plt.scatter(1, (temp_df.price[temp_df.waterfront==1]).mean(),color='indigo')\n",
    "plt.xlabel('Waterfront Property?, 0=No, 1=Yes')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Waterfront View Impact on Mean Price')\n",
    "print('Waterfront Mean Price: ' + str(round((temp_df.price[temp_df.waterfront==1]).mean(),0)))\n",
    "print('No Waterfront Mean Price: ' + str(round((temp_df.price[temp_df.waterfront==0]).mean(),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Filling NaN values did not affect the main conclusion about the waterfront feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 'view' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.view,df.price, color='green', alpha=0.4)\n",
    "plt.xlabel('Views of Property')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.drop('view',axis=1,inplace=True)\n",
    "#features_df.view.fillna(0,inplace=True)       # less than 1% of data was missing.\n",
    "#features_df['view'] = features_df['view'].astype(int).astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 'bedrooms' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.bedrooms,df.price, color='green', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially set as category, changed it back to int\n",
    "\n",
    "#replace type first to INT then to CATEGORY. No variables needed FLOAT type.\n",
    "features_df['bedrooms'] = features_df['bedrooms'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 'floors' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.floors,df.price, color='green', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin features and set as category\n",
    "features_df['floors'] = round(features_df['floors'],0).astype(int)\n",
    "\n",
    "d = {range(0,2):'1', range(2,4):'2plus'}\n",
    "\n",
    "features_df['floors'] = features_df['floors'].apply(lambda x: next((v for k, v in d.items() if x in k), 0))\n",
    "features_df['floors'] = features_df['floors'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 'conditions' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['condition'].hist(color='green',alpha=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin features and set as category\n",
    "\n",
    "features_df['condition'] = features_df['condition'].astype(int)\n",
    "d = {range(0,3):'poor', range(3,4):'average',range(4,6):'above_average'}\n",
    "features_df['condition'] = features_df['condition'].apply(lambda x: next((v for k, v in d.items() if x in k), 0))\n",
    "\n",
    "features_df['condition'] = features_df['condition'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 'zipcode' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['zipcode'] = features_df['zipcode'].astype(int).astype('category')\n",
    "\n",
    "# change a format for basement variable so that all sqft are int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 'grade' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.grade,df.price, color='green', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed from category back to int\n",
    "features_df['grade'] = features_df['grade'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. 'yr_renovated' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.yr_renovated,df.price, color='green', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create categories for renovations based on how recently they occurred.\n",
    "# Assumption: More recent renovations drive the price up more.\n",
    "# There are records of renovations that are more than 30 years old.\n",
    "# For practicality, we will ignore this \"old\" renovations and only\n",
    "# consider a house renovated if it was remodeled within the last 30 years.\n",
    "features_df.yr_renovated.fillna(0,inplace=True)\n",
    "features_df['yr_renovated'] = features_df['yr_renovated'].astype(int)\n",
    "d = {range(0,2000):'no', range(2000, 2015):'recent'}\n",
    "\n",
    "features_df['renovated'] = features_df['yr_renovated'].apply(lambda x: next((v for k, v in d.items() if x in k), 'no'))\n",
    "features_df['renovated'] = features_df['renovated'].astype('category')\n",
    "\n",
    "#drop yr_renovated from features_df\n",
    "features_df.drop('yr_renovated',axis=1,inplace=True)\n",
    "print('Most houses have not been remodeled recently.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. 'yr_built' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature called 'vintage' that will describe how recently\n",
    "# a house was built.\n",
    "d = {range(1900,1970):'Before70s',range(1970, 1980): '70s', range(1980, 1990): '80s', range(1990, 2000): '90s', range(2000, 2010): '2000s',range(2010, 2015): '2010s'}\n",
    "\n",
    "features_df['built'] = features_df['yr_built'].apply(lambda x: next((v for k, v in d.items() if x in k), 0))\n",
    "print('done')\n",
    "features_df['built'] = features_df['built'].astype('category')\n",
    "\n",
    "#drop the year built column, since we categorized it in bins\n",
    "features_df.drop('yr_built',axis=1,inplace=True)\n",
    "features_df['built'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. 'sqft_lot' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(features_df.sqft_lot)\n",
    "plt.show()\n",
    "print ('Skewness =', skew(features_df.sqft_lot))\n",
    "print ('kurtosis =', kurtosis(features_df.sqft_lot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature to deal with lot size\n",
    "# let's check out the distribution\n",
    "features_df.sqft_lot = np.log(features_df.sqft_lot)\n",
    "features_df.sqft_lot.hist(density=True,color='green',alpha=0.4)\n",
    "\n",
    "print ('Skewness =', skew(features_df.sqft_lot))\n",
    "print ('kurtosis =', kurtosis(features_df.sqft_lot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: improved the kurtosis, but it's still a bit high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. and 15. 'lat' and 'long' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using the lat lon variables, approximate the distance of houses to downtown seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a proxy distance from seattle downtown\n",
    "sea_lat  =  47.6050    # seattle's latitude in decimal degrees\n",
    "sea_long = -122.3344   # seattle's longitude ''  ''      ''\n",
    "lat_conv = 69.09       # approximate factor to convert decimal degrees to distance in miles\n",
    "long_conv = 46.54      # gotten from http://www.csgnetwork.com/degreelenllavcalc.html\n",
    "\n",
    "# go through latitude and longitude data and calculate component distance \n",
    "# from seattle.\n",
    "features_df['sea_dist_lat'] = abs(sea_lat - features_df['lat'])*lat_conv\n",
    "features_df['sea_dist_long'] = abs(sea_long - features_df['long'])*long_conv\n",
    "\n",
    "# Using the components, calculate approximate distance using Pythagorean theorem.\n",
    "features_df['approx_dist'] = np.sqrt((features_df['sea_dist_lat'])**2 + (features_df['sea_dist_long'])**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC distance approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calculating distances using this method is a crude approximation.\n",
    "\n",
    "> It is necessary to check whether the results make sense geographically.\n",
    "\n",
    "> Calculations were validated by mapping the data and researching distances manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First QC tool: generate a histogram of distances from the city center\n",
    "# to the various houses. \n",
    "features_df.approx_dist.hist(bins=50,color='green',alpha=.4)\n",
    "plt.xlabel('Distance (mi)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Approximate Distance to Seattle Downtown')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on this histogram, when we plot a heatmap of locations, most\n",
    "# houses should be located within a 10-15 radius of the city center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot heatmap of location values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the folium utility to create a house density map.\n",
    "# we used the original lat and long values to determine whether most\n",
    "# houses are located very near Seattle.\n",
    "\n",
    "def map_values(center_lat, center_long, df):\n",
    "   \n",
    "    m = folium.Map([center_lat,center_long], zoom_start=10)\n",
    "    folium.Marker([center_lat,center_long]).add_to(m)\n",
    "\n",
    "    locations = df[['lat', 'long']]\n",
    "    locationlist = locations.values.tolist()\n",
    "    m.add_children(plugins.HeatMap(locationlist, radius=20, min_opacity=0.25))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_values(sea_lat,sea_long,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, after validating our data using this and outside map\n",
    "# we can include this distance approximation in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat / drop columns \n",
    "features_df.drop('sea_dist_long',axis=1,inplace=True)\n",
    "features_df.drop('sea_dist_lat',axis=1,inplace=True)\n",
    "features_df.drop('lat',axis=1,inplace=True)\n",
    "features_df.drop('long',axis=1,inplace=True)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data info again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create heatmaps and correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.corr()>.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  16 , 17, 18 : 'sqft_living' , 'sqft_above' , 'sqft_living15' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  These variables are correlated over 70% of the time.\n",
    ">  Let's choose only one for our model. The best correlated with price!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlations with target values for correlated variables\n",
    "# to choose the best for our model\n",
    "corr_columns = ['sqft_living','sqft_living15','sqft_above']\n",
    "df_corr = price_df.join(features_df[corr_columns],how='left')\n",
    "df_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on these results, we will nix:  sqft_abovve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.drop('sqft_above',axis=1,inplace=True)\n",
    "#we had dropped more features with a threshhold of .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to float\n",
    "features_df['sqft_living'] = features_df['sqft_living'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 sqft_lot15 and sqft_lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped these before\n",
    "#features_df.drop('sqft_lot15',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remaining correlations\n",
    "features_df.corr()>.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good, no remaining strong correlations!\n",
    "# for now we will keep these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(features_df,figsize  = [9, 9], color='green',alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['bedrooms'] = features_df['bedrooms'].astype('float')\n",
    "features_df['grade'] = features_df['grade'].astype('float')\n",
    "features_df['sqft_basement'] = features_df['sqft_basement'].astype('float')\n",
    "features_df['sqft_living15'] = features_df['sqft_living15'].astype('float')\n",
    "features_df['sqft_lot15'] = features_df['sqft_lot15'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df = features_df.drop(cat_vars, axis=1)\n",
    "\n",
    "#bed= pd.get_dummies(features_df['bedrooms'], prefix=\"bed\")\n",
    "#bath= pd.get_dummies(features_df['bathrooms'], prefix=\"bath\")\n",
    "floor= pd.get_dummies(features_df['floors'], prefix=\"flo\")\n",
    "water= pd.get_dummies(features_df['waterfront'], prefix=\"water\")\n",
    "#view= pd.get_dummies(features_df['view'], prefix=\"view\")\n",
    "cond = pd.get_dummies(features_df['condition'], prefix=\"cond\")\n",
    "#grade= pd.get_dummies(features_df['grade'], prefix=\"grade\")\n",
    "zipc= pd.get_dummies(features_df['zipcode'], prefix=\"zip\")\n",
    "#sold= pd.get_dummies(features_df['yr_sold'], prefix=\"sold\")\n",
    "renov = pd.get_dummies(features_df['renovated'], prefix=\"renov\")\n",
    "built = pd.get_dummies(features_df['built'], prefix=\"built\")\n",
    "#base = pd.get_dummies(features_df['basement'], prefix=\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "features_df = pd.concat([features_df,floor,water,cond,renov,built,zipc],axis=1) #drop bed bath drop grade as test\n",
    "features_df.shape\n",
    "\n",
    "# drop categorical variables written before hot encoding\n",
    "cat_vars = ['floors','waterfront','condition','zipcode','renovated','built']\n",
    "features_df.drop(cat_vars,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize continuous features in dataframe\n",
    "column = 'bedrooms'\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column = 'bathrooms'\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column = 'grade'\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column = 'sqft_basement'\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "\n",
    "column ='sqft_living'\n",
    "features_df[column] = np.log(features_df[column])\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column ='sqft_living15'\n",
    "features_df[column] = np.log(features_df[column])\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column ='approx_dist'\n",
    "features_df[column] = np.log(features_df[column])\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column ='sqft_lot'\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n",
    "\n",
    "column ='sqft_lot15'\n",
    "features_df[column] = (features_df[column] - features_df[column].mean()) / features_df[column].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Linearity Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check distributions for the 3 continuous variables\n",
    "\n",
    "for column in features_df.iloc[:,0:9]:\n",
    "    features_df[column].plot.hist(normed=True,color='green',alpha=0.4,label=column, range=(features_df[column].min(),features_df[column].max()))\n",
    "    features_df[column].plot.kde(label='kde',color='indigo',alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print ('Skewness =', skew(features_df[column]))\n",
    "    print ('kurtosis =', kurtosis(features_df[column]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_price = price_df['price']\n",
    "for column in features_df.iloc[:,0:9]:\n",
    "    sns.jointplot(x=column, y=temp_price,\n",
    "                  data=features_df, \n",
    "                  kind='reg', \n",
    "                  color = 'green',\n",
    "                  label=column,\n",
    "                  joint_kws={'line_kws':{'color':'indigo'}})\n",
    "    sns.regplot(features_df[column], temp_price, label=column, color='green')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run first model and identify opportunities for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Linear Regression for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe with regression information per variable.\n",
    "# df is the dataframe containing features\n",
    "# price_df is the dataframe containing the target\n",
    "# pvalue is the maximum p-value accepted.\n",
    "\n",
    "def run_single_linreg(df, price_df,pvalue):\n",
    "    col_names = df.columns\n",
    "    data_df = price_df.join(df,how='left')\n",
    "\n",
    "    results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value' ]]\n",
    "    for val in (col_names):\n",
    "        f = 'price~' + val\n",
    "        model = smf.ols(formula=f, data=data_df).fit()\n",
    "        X_new = pd.DataFrame({val: [data_df[val].min(), data_df[val].max()]});\n",
    "        preds = model.predict(X_new)\n",
    "        results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1] ])\n",
    "    results_df = pd.DataFrame(results, columns=results[0])\n",
    "    results_df.drop(0,inplace=True)\n",
    "    results_df.set_index('ind_var',inplace=True)\n",
    "    results_df = results_df[results_df['p-value']<=pvalue]\n",
    "    return results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_linreg_1 = run_single_linreg(features_df, price_df,0.05)\n",
    "single_linreg_1.sort_values('r_squared',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTORS: 1st Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected as 1st pass\n",
    "predictors1 = list(single_linreg_1.index)\n",
    "len(predictors1)\n",
    "\n",
    "features_df = features_df[predictors1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multiple Linear Regression Model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes features (predictors) inside a dataframe (df)\n",
    "# and creates the best fit to explain the 'price' (price_df)\n",
    "\n",
    "def run_mult_linreg(df, price_df, predictors):\n",
    "    y = price_df.price\n",
    "    X = df[predictors]\n",
    "    n_predictors = len(predictors)\n",
    "    linreg = LinearRegression()\n",
    "    r_list = []\n",
    "    adj_r_list = []\n",
    "    list_n = list(range(n_predictors//2,n_predictors))  #n predictors calculated above\n",
    "    for n in list_n: \n",
    "        select_n = RFE(linreg, n_features_to_select = n)\n",
    "        select_n = select_n.fit(X, np.ravel(y))\n",
    "        selected_columns = X.columns[select_n.support_ ]\n",
    "        linreg.fit(X[selected_columns],y)\n",
    "        yhat = linreg.predict(X[selected_columns])\n",
    "        SS_Residual = np.sum((y-yhat)**2)\n",
    "        SS_Total = np.sum((y-np.mean(y))**2)\n",
    "        r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "        print(r_squared)\n",
    "        adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "    r_list.append(r_squared)\n",
    "    adj_r_list.append(adjusted_r_squared)\n",
    "    print('r-squared: ' + str(r_list))\n",
    "    print('adj r-squared: ' + str(adj_r_list))\n",
    "    return linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the first linear regression and print r-squared values\n",
    "mult_linreg1 = run_mult_linreg(features_df,price_df,predictors1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(linreg,df, price_df, predictors,cv_number):\n",
    "    y = price_df.price\n",
    "    X = df[predictors]\n",
    "    n_predictors = len(list(predictors))\n",
    "    select_n = RFE(linreg, n_features_to_select = n_predictors)\n",
    "    select = select_n.fit(X, np.ravel(y))\n",
    "    selected_columns = X.columns[select_n.support_]\n",
    "\n",
    "    cv_10_results = cross_val_score(linreg, X[selected_columns], y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "    print(str(n_predictors) + ' predictors used.')\n",
    "    print(cv_10_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validation(mult_linreg1,features_df,price_df,predictors1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large cross-validation values indicate multicollinearity issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ols_model(df, price_df,predictors):\n",
    "    data_df = price_df.join(df,how='left')\n",
    "    outcome = 'price'\n",
    "\n",
    "    data_df.info()\n",
    "    predictors_list = '+'.join(list(predictors))\n",
    "\n",
    "    formula = outcome + \"~\" + predictors_list\n",
    "    model = ols(formula=formula, data=data_df).fit()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_1 = run_ols_model(features_df,price_df,predictors1)\n",
    "ols_model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observation: There are multicollinearity issues... Need to remove binary categories or other correlated variables.\n",
    "\n",
    "> Bedrooms is negatively correlated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(ols_model_1.resid, dist=stats.norm, line=('45'), fit=True,color='green',alpha=.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variables behave linearly for about 2 standard deviations from the mean in each direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df1 = price_df.join(features_df,how='left')\n",
    "plt.scatter(ols_model_1.predict(data_df1), ols_model_1.resid,color='green',alpha=.4)\n",
    "plt.plot(ols_model_1.predict(data_df1), [0 for i in range(len(data_df1))],color='indigo',alpha=.6)\n",
    "                                                      \n",
    "                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Overall this is a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ITERATION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Variable Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bedrooms vs. Living Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.bedrooms,features_df.sqft_living,color='green',alpha=0.4)\n",
    "plt.xlabel('Bedrooms (normalized)')\n",
    "plt.ylabel('Living Area (sqft)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datapoint (33 bedrooms) looks like a typing error. Maybe they meant 3 bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's find out this datapoint in the dataframe\n",
    "features_df.loc[df['bedrooms'] == 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['bedrooms'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['bedrooms'] == 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear this was a typo, a house with 1620 sqft cannot have 33 bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace this value with the log value for 3, which is the median\n",
    "value = (3 - df.bedrooms.mean()) / df.bedrooms.std()\n",
    "features_df.loc[2402100895,'bedrooms'] = value\n",
    "features_df.loc[1773100755,'bedrooms'] = value\n",
    "features_df.loc[5566100170,'bedrooms'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.bedrooms,features_df.sqft_living, color='green',alpha=0.4)\n",
    "plt.xlabel('Corrected # Bedrooms')\n",
    "plt.ylabel('Living Area (sqft)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Binary / Singular Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.drop('water_0',axis=1,inplace=True)  # Remove no waterfront\n",
    "features_df.drop('renov_no',axis=1,inplace=True) # Remove no renovations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_df.drop('flo_1',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(features_df.corr().iloc[0:10,0:10], center=0, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.corr().iloc[0:30]>.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.drop('bathrooms',axis=1,inplace=True)\n",
    "features_df.drop('bedrooms',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.drop('sqft_living15',axis=1,inplace=True)\n",
    "features_df.drop('grade',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(features_df.corr(), center=0, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Second Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg2 = run_single_linreg(features_df, price_df,0.05)\n",
    "predictors2 = list(linreg2.index)\n",
    "features_df = features_df[predictors2]\n",
    "len(predictors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_linreg2 = run_mult_linreg(features_df, price_df, predictors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validation(mult_linreg2,features_df,price_df,predictors2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_2 = run_ols_model(features_df,price_df,predictors2)\n",
    "ols_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(ols_model_2.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(features_df.corr(), center=0, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 3rd Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors3 = stepwise_selection(features_df, price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictors3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run 3rd Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df[predictors3]\n",
    "#Run the third linear regression and print r-squared values\n",
    "mult_linreg3 = run_mult_linreg(features_df,price_df,predictors3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validation(mult_linreg3,features_df,price_df,predictors3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_2 = run_ols_model(features_df,price_df,predictors3)\n",
    "ols_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check on correlated variables\n",
    "find_corr_vars(features_df,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features_df.sqft_basement,price_df.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC: Train/Test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = features_df[predictors3]\n",
    "y = price_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=3)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "MSE_test = []\n",
    "MSE_train = []\n",
    "list_n = list(range(6,86,10))\n",
    "for n in list_n: \n",
    "    select_n = RFE(linreg, n_features_to_select = n)\n",
    "    select_n = select_n.fit(X_train, np.ravel(y_train))\n",
    "    selected_columns = X.columns[select_n.support_ ]\n",
    "    linreg.fit(X_train[selected_columns],y_train)\n",
    "    yhat_train = linreg.predict(X_train[selected_columns])\n",
    "    yhat_test = linreg.predict(X_test[selected_columns])\n",
    "    mse_train = np.sum((y_train-yhat_train)**2)/len(y_train)\n",
    "    mse_test =np.sum((y_test-yhat_test)**2)/len(y_test)\n",
    "    print(mse_train)\n",
    "    print(mse_test)\n",
    "MSE_test.append(mse_test)\n",
    "MSE_train.append(mse_train)\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = price_df.join(features_df,how='left')\n",
    "data_df.corr().loc['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors2 = stepwise_selection(features_df, price_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
